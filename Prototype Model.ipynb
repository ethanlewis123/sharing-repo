{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, Embedding, LSTM, Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import math\n",
    "import errno\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "# Scheduling a learning-rate to produce different effects for gradient of loss wrt to the weights\n",
    "# Different Learning Rates will affect the model differently by updating different % of weights of the model.\n",
    "START_CHAR = \"\\t\"\n",
    "END_CHAR = \"\\n\"\n",
    "LSTM_UNITS = 300\n",
    "DENSE_UNITS = 100\n",
    "VOCAB_SIZE = 70\n",
    "PROB_THRESHOLD = 1e-9\n",
    "VERBOSITY = 2\n",
    "EPOCHS = 2\n",
    "MAX_LENGTH = 32\n",
    "EMBEDDING_DIMENSION = 16\n",
    "BATCH_SIZE = 32\n",
    "METRICS = [tf.keras.metrics.CategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing the inputs and the outputs for the model --- \n",
    "# For Example password 'passwd'\n",
    "# Adding a \\t at the Start for the input - \\tpasswd \n",
    "# Adding a \\n at the End for the model output - passwd\\n\n",
    "# Here \\t predicts p, p predicts a and so on..\n",
    "def loadPreprocessInputOutput(FILE_NAME, START_CHAR, END_CHAR):\n",
    "    try:\n",
    "        assert os.path.isfile(FILE_NAME)\n",
    "        assert isinstance(START_CHAR, str)\n",
    "        assert isinstance(END_CHAR, str)\n",
    "    except:\n",
    "        raise Exception(\"Incorrect Inputs. Try again.\")\n",
    "        return\n",
    "    inputPasswords = []\n",
    "    outputPasswords = []\n",
    "    listPasswords = []\n",
    "    with open(FILE_NAME, \"r\") as pass_file:\n",
    "        while(True):\n",
    "            single_pass = pass_file.readline().rstrip(\"\\n\")\n",
    "            if(single_pass == \"\"):\n",
    "                break\n",
    "            else:\n",
    "                inputPasswords.append(START_CHAR + single_pass)\n",
    "                outputPasswords.append(single_pass + END_CHAR)\n",
    "                listPasswords.append(START_CHAR + single_pass + END_CHAR)\n",
    "        return(inputPasswords, outputPasswords, listPasswords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputPasswords, outPasswords, listPasswords = loadPreprocessInputOutput(\"/home/rm/BE_Project/Embedding/Data/ascii_rockyou_less_than_thirty_two_cleaned.txt\", START_CHAR, END_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "print(inputPasswords[5:10])\n",
    "print(len(inputPasswords))\n",
    "print(outPasswords[5:10])\n",
    "print(len(outPasswords))\n",
    "print(listPasswords[5:10])\n",
    "print(len(listPasswords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the passwords:\n",
    "# Do not open file - will not be shown properly\n",
    "def writePreprocessedPasswordFromList(passwordList, typeList):\n",
    "    passwordFileName = typeList + \"_preprocessed.txt\"\n",
    "    with open(passwordFileName, \"w\") as pass_file:\n",
    "        for password in passwordList:\n",
    "            if(typeList.lower() == \"input\"):\n",
    "                pass_file.write(password+\"\\n\")\n",
    "            else:\n",
    "                pass_file.write(password)\n",
    "    print(f\"{typeList} passwords written completely to : {passwordFileName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write the passwords\n",
    "writePreprocessedPasswordFromList(inputPasswords, \"input\")\n",
    "writePreprocessedPasswordFromList(outPasswords, \"output\")\n",
    "writePreprocessedPasswordFromList(listPasswords, \"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t123456\n",
      "\t12345\n",
      "\t123456789\n",
      "\tpassword\n",
      "\tiloveyou\n",
      "123456\n",
      "12345\n",
      "123456789\n",
      "password\n",
      "iloveyou\n",
      "\t123456\n",
      "\t12345\n",
      "\t123456789\n",
      "\tpassword\n",
      "\tiloveyou\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "# Output will not be proper.\n",
    "!head -n 5 input_preprocessed.txt\n",
    "!head -n 5 output_preprocessed.txt\n",
    "!head -n 5 list_preprocessed.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specifying the Tokenizer -- leaving num_words as blank to include as many \n",
    "# unique characters as possible.\n",
    "# Fit the tokenizer on the text and then save the tokenizer.\n",
    "passwordTokenizer = Tokenizer(filters = \"\", lower = True, char_level = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility Function to Save the Tokenizer Configuration\n",
    "def saveTokenizer(TOKENIZER, OUTPUT_PATH):\n",
    "    tokenizerConfigString = TOKENIZER.to_json()\n",
    "    with open(OUTPUT_PATH+\".json\", \"w\") as op_file:\n",
    "        op_file.write(tokenizerConfigString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility Function to Load the Tokenizer Configuration\n",
    "def loadTokenizer(TOKENIZER_FILE_PATH):\n",
    "    _, file_extension = os.path.splitext(TOKENIZER_FILE_PATH)\n",
    "    if(file_extension != \".json\"):\n",
    "        raise Exception(\"Incorrect File.\")\n",
    "        return\n",
    "    else:\n",
    "        with open(TOKENIZER_FILE_PATH, \"r\") as tokenizer_cfg_file:\n",
    "            tokenizer_config = tokenizer_cfg_file.read()\n",
    "            tokenizer_cfg = json.loads(tokenizer_config)\n",
    "            passwordTokenizer = tf.keras.preprocessing.text.tokenizer_from_json(json.dumps(tokenizer_cfg))\n",
    "            return passwordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fitting the Tokenizer on data:\n",
    "passwordTokenizer.fit_on_texts(listPasswords)\n",
    "passwordTokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save Tokenizer -- for later use and load it to avoid re-fitting\n",
    "saveTokenizer(passwordTokenizer, \"prototypeTokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_words': None,\n",
       " 'filters': '',\n",
       " 'lower': True,\n",
       " 'split': ' ',\n",
       " 'char_level': True,\n",
       " 'oov_token': None,\n",
       " 'document_count': 14105697,\n",
       " 'word_counts': '{\"\\\\t\": 14105697, \"1\": 6729506, \"2\": 5234401, \"3\": 3765169, \"4\": 3389487, \"5\": 3352338, \"6\": 3116090, \"\\\\n\": 14105697, \"7\": 3098762, \"8\": 3565308, \"9\": 3853241, \"p\": 1619704, \"a\": 8828625, \"s\": 4154066, \"w\": 799568, \"o\": 5173138, \"r\": 4576620, \"d\": 2484237, \"i\": 5553157, \"l\": 4460473, \"v\": 1050793, \"e\": 7203479, \"y\": 2373398, \"u\": 2307207, \"n\": 4827931, \"c\": 2608350, \"k\": 2012166, \"b\": 2110821, \"g\": 1717216, \"m\": 3205286, \"j\": 1237461, \"h\": 2335137, \"q\": 178503, \"t\": 3425154, \"0\": 5735322, \"f\": 981496, \"z\": 763424, \"x\": 479316, \"!\": 142923, \";\": 12258, \"-\": 133010, \"*\": 123842, \".\": 248717, \"?\": 18301, \",\": 29750, \"/\": 48190, \"#\": 48873, \"@\": 107880, \"$\": 36029, \"%\": 10254, \"^\": 6394, \"&\": 26359, \"+\": 26989, \"\\'\": 15335, \"[\": 7682, \"]\": 10802, \"<\": 9561, \"_\": 193006, \">\": 2458, \"=\": 18365, \"\\\\\\\\\": 25832, \"\\\\\"\": 3637, \":\": 6858, \"(\": 17037, \")\": 18963, \"`\": 5451, \"~\": 8327, \"{\": 1050, \"}\": 964, \"|\": 700}',\n",
       " 'word_docs': '{\"4\": 2700425, \"6\": 2447173, \"3\": 2998313, \"1\": 5049998, \"\\\\n\": 14105697, \"2\": 4021581, \"\\\\t\": 14105697, \"5\": 2601993, \"8\": 2736136, \"9\": 2888581, \"7\": 2454981, \"w\": 746175, \"d\": 2182950, \"s\": 3435023, \"a\": 6203639, \"p\": 1391233, \"r\": 3920485, \"o\": 3969565, \"e\": 5325265, \"y\": 2163861, \"u\": 2070765, \"v\": 993992, \"l\": 3576048, \"i\": 4540656, \"n\": 3967300, \"c\": 2306277, \"k\": 1806549, \"b\": 1753293, \"g\": 1530627, \"m\": 2763367, \"j\": 1143502, \"h\": 2118370, \"t\": 2885245, \"q\": 168747, \"0\": 3810122, \"f\": 863905, \"z\": 665326, \"x\": 404231, \"!\": 116204, \";\": 10751, \"-\": 107808, \"*\": 80989, \".\": 187176, \"?\": 14422, \",\": 24512, \"/\": 33359, \"#\": 44736, \"%\": 9134, \"@\": 96424, \"$\": 26877, \"^\": 4795, \"&\": 25006, \"+\": 22134, \"\\'\": 14068, \"]\": 9497, \"[\": 6831, \"<\": 8752, \"_\": 170266, \">\": 1983, \"=\": 16351, \"\\\\\\\\\": 8027, \"\\\\\"\": 2352, \":\": 5873, \")\": 16989, \"(\": 15338, \"`\": 4627, \"~\": 5363, \"}\": 878, \"{\": 985, \"|\": 490}',\n",
       " 'index_docs': '{\"18\": 2700425, \"21\": 2447173, \"15\": 2998313, \"5\": 5049998, \"2\": 14105697, \"8\": 4021581, \"1\": 14105697, \"19\": 2601993, \"16\": 2736136, \"14\": 2888581, \"22\": 2454981, \"35\": 746175, \"24\": 2182950, \"13\": 3435023, \"3\": 6203639, \"31\": 1391233, \"11\": 3920485, \"9\": 3969565, \"4\": 5325265, \"25\": 2163861, \"27\": 2070765, \"33\": 993992, \"12\": 3576048, \"7\": 4540656, \"10\": 3967300, \"23\": 2306277, \"29\": 1806549, \"28\": 1753293, \"30\": 1530627, \"20\": 2763367, \"32\": 1143502, \"26\": 2118370, \"17\": 2885245, \"40\": 168747, \"6\": 3810122, \"34\": 863905, \"36\": 665326, \"37\": 404231, \"41\": 116204, \"57\": 10751, \"42\": 107808, \"43\": 80989, \"38\": 187176, \"54\": 14422, \"48\": 24512, \"46\": 33359, \"45\": 44736, \"59\": 9134, \"44\": 96424, \"47\": 26877, \"64\": 4795, \"50\": 25006, \"49\": 22134, \"56\": 14068, \"58\": 9497, \"62\": 6831, \"60\": 8752, \"39\": 170266, \"67\": 1983, \"53\": 16351, \"51\": 8027, \"66\": 2352, \"63\": 5873, \"52\": 16989, \"55\": 15338, \"65\": 4627, \"61\": 5363, \"69\": 878, \"68\": 985, \"70\": 490}',\n",
       " 'index_word': '{\"1\": \"\\\\t\", \"2\": \"\\\\n\", \"3\": \"a\", \"4\": \"e\", \"5\": \"1\", \"6\": \"0\", \"7\": \"i\", \"8\": \"2\", \"9\": \"o\", \"10\": \"n\", \"11\": \"r\", \"12\": \"l\", \"13\": \"s\", \"14\": \"9\", \"15\": \"3\", \"16\": \"8\", \"17\": \"t\", \"18\": \"4\", \"19\": \"5\", \"20\": \"m\", \"21\": \"6\", \"22\": \"7\", \"23\": \"c\", \"24\": \"d\", \"25\": \"y\", \"26\": \"h\", \"27\": \"u\", \"28\": \"b\", \"29\": \"k\", \"30\": \"g\", \"31\": \"p\", \"32\": \"j\", \"33\": \"v\", \"34\": \"f\", \"35\": \"w\", \"36\": \"z\", \"37\": \"x\", \"38\": \".\", \"39\": \"_\", \"40\": \"q\", \"41\": \"!\", \"42\": \"-\", \"43\": \"*\", \"44\": \"@\", \"45\": \"#\", \"46\": \"/\", \"47\": \"$\", \"48\": \",\", \"49\": \"+\", \"50\": \"&\", \"51\": \"\\\\\\\\\", \"52\": \")\", \"53\": \"=\", \"54\": \"?\", \"55\": \"(\", \"56\": \"\\'\", \"57\": \";\", \"58\": \"]\", \"59\": \"%\", \"60\": \"<\", \"61\": \"~\", \"62\": \"[\", \"63\": \":\", \"64\": \"^\", \"65\": \"`\", \"66\": \"\\\\\"\", \"67\": \">\", \"68\": \"{\", \"69\": \"}\", \"70\": \"|\"}',\n",
       " 'word_index': '{\"\\\\t\": 1, \"\\\\n\": 2, \"a\": 3, \"e\": 4, \"1\": 5, \"0\": 6, \"i\": 7, \"2\": 8, \"o\": 9, \"n\": 10, \"r\": 11, \"l\": 12, \"s\": 13, \"9\": 14, \"3\": 15, \"8\": 16, \"t\": 17, \"4\": 18, \"5\": 19, \"m\": 20, \"6\": 21, \"7\": 22, \"c\": 23, \"d\": 24, \"y\": 25, \"h\": 26, \"u\": 27, \"b\": 28, \"k\": 29, \"g\": 30, \"p\": 31, \"j\": 32, \"v\": 33, \"f\": 34, \"w\": 35, \"z\": 36, \"x\": 37, \".\": 38, \"_\": 39, \"q\": 40, \"!\": 41, \"-\": 42, \"*\": 43, \"@\": 44, \"#\": 45, \"/\": 46, \"$\": 47, \",\": 48, \"+\": 49, \"&\": 50, \"\\\\\\\\\": 51, \")\": 52, \"=\": 53, \"?\": 54, \"(\": 55, \"\\'\": 56, \";\": 57, \"]\": 58, \"%\": 59, \"<\": 60, \"~\": 61, \"[\": 62, \":\": 63, \"^\": 64, \"`\": 65, \"\\\\\"\": 66, \">\": 67, \"{\": 68, \"}\": 69, \"|\": 70}'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passwordTokenizer = loadTokenizer(\"prototypeTokenizer.json\")\n",
    "# Sanity Check\n",
    "passwordTokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLength(FILE_PATH):\n",
    "    count = 0\n",
    "    with open(FILE_PATH, \"r\") as f:\n",
    "        for count, _ in enumerate(f):\n",
    "            pass\n",
    "    count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to preprocess and get validation data\n",
    "def getValidationData(VALID_X_FILE_PATH, VALID_Y_FILE_PATH, VOCAB_SIZE, TOKENIZER, MAX_LENGTH, VALIDATION_BATCH_SIZE):\n",
    "    total_passwords = getLength(VALID_X_FILE_PATH)\n",
    "    total_batches = math.floor(total_passwords / VALIDATION_BATCH_SIZE)\n",
    "    total_passwords_to_read = total_batches * VALIDATION_BATCH_SIZE\n",
    "    valid_passwords = []\n",
    "    count = 0\n",
    "    valid_y = []\n",
    "    flag = True\n",
    "    with open(VALID_X_FILE_PATH, \"r\") as valid_file:\n",
    "        for _, password in enumerate(valid_file):\n",
    "            if(_ < total_passwords_to_read):\n",
    "                valid_passwords.append(password.rstrip(\"\\n\"))\n",
    "    with open(VALID_Y_FILE_PATH, \"r\") as valid_true_file:\n",
    "        for _, true_password in enumerate(valid_true_file):\n",
    "            if(_ < total_passwords_to_read):\n",
    "                valid_y.append(true_password)\n",
    "    valid_encoded_passwords = TOKENIZER.texts_to_sequences(valid_passwords)\n",
    "    valid_y_encoded_passwords = TOKENIZER.texts_to_sequences(valid_y)\n",
    "    valid_padded_passwords = pad_sequences(valid_encoded_passwords, padding = \"post\", maxlen = (MAX_LENGTH + 1))\n",
    "    valid_y_padded_passwords = pad_sequences(valid_y_encoded_passwords, padding = \"post\", maxlen = (MAX_LENGTH + 1))\n",
    "    for x_valid_password, y_valid_password in zip(valid_padded_passwords, valid_y_padded_passwords):\n",
    "        count += 1\n",
    "        if(count % 500 == 0):\n",
    "            print(f\"Total passwords processed {count}\")\n",
    "        if(flag):\n",
    "            final_x_valid = np.array(x_valid_password).reshape(1, (MAX_LENGTH + 1))\n",
    "            temp_y_valid = np.array(y_valid_password).reshape(1, (MAX_LENGTH + 1))\n",
    "            final_y_valid = to_categorical(temp_y_valid, num_classes = (VOCAB_SIZE + 1))\n",
    "            flag = False\n",
    "        else:\n",
    "            final_x_valid = np.concatenate((final_x_valid, np.array(x_valid_password).reshape(1, (MAX_LENGTH + 1))), axis = 0)\n",
    "            temp_y_valid = np.array(y_valid_password).reshape(1, (MAX_LENGTH + 1))                                 \n",
    "            final_y_valid = np.concatenate((final_y_valid, to_categorical(temp_y_valid, num_classes = (VOCAB_SIZE + 1))), axis = 0)\n",
    "    return(final_x_valid, final_y_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an input pipeline for feeding encoded passwords -- \n",
    "# Pass train_passwords as None if you want to train on entire set of passwords mentioned in password_file\n",
    "class lstmNetworkInputSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, train_passwords, password_file_x, password_file_y, batch_size, tokenizer, max_length, vocab_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.train_passwords = train_passwords\n",
    "        self.password_file = password_file_x\n",
    "        self.password_file_y = password_file_y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        with open(self.password_file, \"r\") as pass_file:\n",
    "            for count, _ in enumerate(pass_file):\n",
    "                pass\n",
    "        total_passwords = count + 1\n",
    "        if(self.train_passwords is not None):\n",
    "            if(self.train_passwords < total_passwords):\n",
    "                return math.floor(self.train_passwords / self.batch_size)\n",
    "            else:\n",
    "                return math.floor(total_passwords / self.batch_size)\n",
    "        else:\n",
    "            return math.floor(total_passwords / self.batch_size)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # This could be slow, will try to improve speed later by saving encoded passwords.\n",
    "        # batch_enc_padded_passwords = None\n",
    "        # batch_y_true = None\n",
    "        flag = True\n",
    "        batch_enc_padded_passwords = []\n",
    "        batch_y_true = []\n",
    "        # temp_x = []\n",
    "        # temp_y = []\n",
    "        batch_passwords = []\n",
    "        batch_y_passwords = []\n",
    "        batch_password_index = list(range((index - 1) * self.batch_size, ((index) * self.batch_size)))\n",
    "        with open(self.password_file, \"r\") as pass_file:\n",
    "            for count, password in enumerate(pass_file):\n",
    "                if(count in batch_password_index):\n",
    "                    batch_passwords.append(password.rstrip(\"\\n\"))\n",
    "                else:\n",
    "                    continue\n",
    "        with open(self.password_file_y, \"r\") as pass_file_y:\n",
    "            for count_y, password_y in enumerate(pass_file_y):\n",
    "                if(count_y in batch_password_index):\n",
    "                    batch_y_passwords.append(password_y)\n",
    "                else:\n",
    "                    continue\n",
    "        #print(f\"{batch_passwords}\\n\\n\\n{batch_y_passwords}\")\n",
    "        encoded_passwords = self.tokenizer.texts_to_sequences(batch_passwords)\n",
    "        padded_encoded_passwords = pad_sequences(encoded_passwords, padding = \"post\", maxlen = (self.max_length + 1))\n",
    "        encoded_y_passwords = self.tokenizer.texts_to_sequences(batch_y_passwords)\n",
    "        padded_encoded_y_passwords = pad_sequences(encoded_y_passwords, padding = \"post\", maxlen = (self.max_length + 1))\n",
    "        for encoded_password, encoded_y_password in zip(padded_encoded_passwords, padded_encoded_y_passwords):\n",
    "            reshaped_example = np.array(encoded_password).reshape(1, (self.max_length + 1))\n",
    "            reshaped_y_example = np.array(encoded_y_password).reshape(1, (self.max_length + 1))\n",
    "            if(flag):\n",
    "                batch_enc_padded_passwords = reshaped_example\n",
    "                batch_y_true = to_categorical(y = reshaped_y_example, num_classes = (self.vocab_size + 1))\n",
    "                flag = False\n",
    "            else:\n",
    "                batch_enc_padded_passwords = np.concatenate((batch_enc_padded_passwords, reshaped_example), axis = 0)\n",
    "                batch_y_true = np.concatenate((batch_y_true, to_categorical(y = reshaped_y_example,  num_classes = (self.vocab_size + 1))), axis = 0)\n",
    "        return np.array(batch_enc_padded_passwords), np.array(batch_y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputPasswords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-86633b5b58aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Resource intensive and will slow us down considerably.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 7. train_test_split is given the input and output arrays as X & Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX_train_test_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_test_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputPasswords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutPasswords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_test_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_test_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputPasswords' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Preparing the Data to splitting into train and validation\n",
    "# 2. Then prepare the train Data to split again into train and test\n",
    "# 3. Don't train on test \n",
    "# 4. Modify hyperparameters on validate\n",
    "# 5. Plot using TensorBoard for both train and validation for each epoch \n",
    "# 6. Necessary to train if for atleast one epoch, to plot the graphs\n",
    "# Or we can choose to plot for each batch - but will be very\n",
    "# Resource intensive and will slow us down considerably.\n",
    "# 7. train_test_split is given the input and output arrays as X & Y\n",
    "X_train_test_train, X_validation, Y_train_test_train, Y_validation = train_test_split(inputPasswords, outPasswords, test_size = 0.05, shuffle = True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train_test_train, Y_train_test_train, test_size = 0.1, shuffle = True)\n",
    "\n",
    "# Sanity Check\n",
    "print(f\"{repr(X_train_test_train[100])}\\t{repr(Y_train_test_train[100])}\")\n",
    "print(f\"{repr(X_validation[100])}\\t{repr(Y_validation[100])}\")\n",
    "print(f\"{repr(X_train[100])}\\t{repr(Y_train[100])}\")\n",
    "print(f\"{repr(X_test[100])}\\t{repr(Y_test[100])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing to save models - dedicated methods for it\n",
    "# We also save them after each epoch using checkpoint callback.\n",
    "# So saving manually is optional.\n",
    "def saveKerasModel(MODEL, OUTPUT_MODEL_PATH):\n",
    "    # Saves the model to the disk, saves both the architecture and \n",
    "    # the configuration.\n",
    "    try:\n",
    "        assert isinstance(MODEL, Model)\n",
    "        MODEL.save(OUTPUT_MODEL_PATH)\n",
    "        print(f\"[+] Model has been successfully saved to {OUTPUT_MODEL_PATH}\")\n",
    "    except:\n",
    "        raise Exception(\"Model instance is incorrect. Failed!\")\n",
    "        return   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility Function to load the model manually.\n",
    "# Is Extremely Important!\n",
    "def loadKerasModel(INPUT_MODEL_PATH):\n",
    "    try:\n",
    "        assert os.path.isfile(INPUT_MODEL_PATH)\n",
    "        loaded_model = tf.keras.load_model(INPUT_MODEL_PATH)\n",
    "    except:\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), INPUT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir train\n",
    "!mkdir test\n",
    "!mkdir validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility Function to write the output to file -- \n",
    "def writeOutput(FILE_PATH, FILE_TYPE_TRAIN, PASS_LIST_X, PASS_LIST_Y):\n",
    "    with open(os.path.join(FILE_PATH, FILE_TYPE_TRAIN + \"_X\" + \".txt\"), \"w\") as x_file:\n",
    "        for input_password in PASS_LIST_X:\n",
    "            x_file.write(input_password + \"\\n\")\n",
    "    with open(os.path.join(FILE_PATH, FILE_TYPE_TRAIN + \"_Y\" + \".txt\"), \"w\") as y_file:\n",
    "        for out_password in PASS_LIST_Y:\n",
    "            y_file.write(out_password)\n",
    "    print(\"[+] Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the split data into their respective directories --\n",
    "writeOutput(\"./train\", \"train\", X_train, Y_train)\n",
    "writeOutput(\"./test\", \"test\", X_test, Y_test)\n",
    "writeOutput(\"./validation\", \"validation\", X_validation, Y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\trissaab\n",
      "\t0835777414\n",
      "rissaab\n",
      "0835777414\n",
      "\tebdevils24\n",
      "\tcatarro\n",
      "ebdevils24\n",
      "catarro\n",
      "\txarahj\n",
      "\tchaus19\n",
      "xarahj\n",
      "chaus19\n",
      "12060370 ./train/train_X.txt\n",
      "12060370 ./train/train_Y.txt\n",
      "1340042 ./test/test_X.txt\n",
      "1340042 ./test/test_Y.txt\n",
      "705285 ./validation/validation_X.txt\n",
      "705285 ./validation/validation_Y.txt\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check \n",
    "!head -n 2 ./train/train_X.txt \n",
    "!head -n 2 ./train/train_Y.txt\n",
    "!head -n 2 ./test/test_X.txt \n",
    "!head -n 2 ./test/test_Y.txt \n",
    "!head -n 2 ./validation/validation_X.txt \n",
    "!head -n 2 ./validation/validation_Y.txt \n",
    "!wc -l ./train/train_X.txt \n",
    "!wc -l ./train/train_Y.txt \n",
    "!wc -l ./test/test_X.txt\n",
    "!wc -l ./test/test_Y.txt\n",
    "!wc -l ./validation/validation_X.txt \n",
    "!wc -l ./validation/validation_Y.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing the file directory structure for callbacks:\n",
    "!mkdir checkpoints\n",
    "!mkdir tensorboard_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configuring all model callbacks\n",
    "# 1. LearningRate Callback\n",
    "# 2. ModelCheckpoint Callback\n",
    "# 3. TensorBoard Callback\n",
    "# 4. GradientClipping Callback\n",
    "# Gradient Clipping is required because LSTMs/GRUs greatly suffer from both\n",
    "# Exploding and Vanishing Gradients which presents as big problem and sets\n",
    "# all values of delta-wt, delta-b to NaN causing numerical instability.\n",
    "# This occurs because of Gradient-Descent through Time or as we call it\n",
    "# Back-Propogation through time algorithm causing the gradients to be multiplied \n",
    "# with <1 or >1 weight values across all timesteps leaving the network extremely\n",
    "# vulnerable to numerical instability.\n",
    "\n",
    "# LearningRateScheduler - Using Exponential Learning Rate Decay\n",
    "# First defining a 'schedule' for learning rate decay\n",
    "def expLearningRateDecay(epoch):\n",
    "   initial_lrate = 0.1\n",
    "   k = 0.1\n",
    "   lrate = initial_lrate * math.exp(-k * epoch)\n",
    "   return lrate\n",
    "\n",
    "# Scheduler Callback:\n",
    "train_learning_rate_callback = LearningRateScheduler(schedule = expLearningRateDecay, verbose = 1)\n",
    "\n",
    "# Checkpoint Callback:\n",
    "train_checkpoint_callback = ModelCheckpoint(\"./checkpoints/Checkpoint-{epoch:03d}\", verbose = 1, save_weights_only = False, save_freq = \"epoch\")\n",
    "\n",
    "# TensorBoard Callback:\n",
    "train_tensorboard_callback = TensorBoard(log_dir = \"./tensorboard_log_dir\", histogram_freq = 1, write_graph = True, write_images = True, update_freq = 'batch')\n",
    "\n",
    "# GradientClipping Callback\n",
    "\n",
    "# GradientClipping has to be done manually by computing the gradient wrt \n",
    "# the loss of each trainable parameter and then clip it.\n",
    "# After which the gradients have to be applied to the weights / biases / gates\n",
    "# thus preventing exploding/vanishing gradient.\n",
    "\n",
    "# This will only be done if the model experiences any sort of numerical \n",
    "# instability and throws NaN Exceptions since this process has capacity \n",
    "# to cripple model's speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating Optimizer Instance for Training Model\n",
    "adam_optimizer = Adam(learning_rate = LEARNING_RATE, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building teacher-forcing training model\n",
    "# Must include shared architecture for following layers --\n",
    "# 1. Embedding Layer\n",
    "# 2. LSTM_1 \n",
    "# 3. LSTM_2\n",
    "# 4. LSTM_3\n",
    "# 5. Dense_1\n",
    "# 6. Dense_2 (Softmax - Outputs Probability Distribution over |VOCAB_SIZE + <PAD_TOKEN>|)\n",
    "\n",
    "# The shared layers are defined below -- \n",
    "shared_embedding_layer = Embedding(input_dim = (VOCAB_SIZE + 1), output_dim = EMBEDDING_DIMENSION, mask_zero = True)\n",
    "shared_lstm_1 = LSTM(LSTM_UNITS, return_sequences = True, return_state = True)\n",
    "shared_lstm_2 = LSTM(LSTM_UNITS, return_sequences = True, return_state = True)\n",
    "shared_lstm_3 = LSTM(LSTM_UNITS, return_sequences = True, return_state = True)\n",
    "shared_dense_1 = TimeDistributed(Dense(DENSE_UNITS, activation = \"relu\"))\n",
    "shared_dense_op = TimeDistributed(Dense((VOCAB_SIZE + 1), activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_57 (InputLayer)        [(None, 33)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 33, 16)            1136      \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               [(None, 33, 300), (None,  380400    \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               [(None, 33, 300), (None,  721200    \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               [(None, 33, 300), (None,  721200    \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 33, 100)           30100     \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 33, 71)            7171      \n",
      "=================================================================\n",
      "Total params: 1,861,207\n",
      "Trainable params: 1,861,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 1. Defining the Teacher-Force Training Model:\n",
    "# 2. Do not use initial_state call argument for the LSTM layers\n",
    "# 3. Use the initial_state call arguemnt for 'all' the LSTM layers \n",
    "# in the Inference Model\n",
    "\n",
    "train_input = Input(shape = ((MAX_LENGTH + 1),))\n",
    "train_emb_op = shared_embedding_layer(train_input)\n",
    "train_lstm_1_op, train_lstm_1_hidden, train_lstm_1_cell = shared_lstm_1(train_emb_op)\n",
    "train_lstm_2_op, train_lstm_2_hidden, train_lstm_2_cell = shared_lstm_2(train_lstm_1_op)\n",
    "train_lstm_3_op, train_lstm_3_hidden, train_lstm_3_cell = shared_lstm_3(train_lstm_2_op)\n",
    "train_dense_1_op = shared_dense_1(train_lstm_3_op)\n",
    "train_model_op = shared_dense_op(train_dense_1_op)\n",
    "\n",
    "train_model = Model(inputs = train_input, outputs = train_model_op)\n",
    "\n",
    "# Check the model summary \n",
    "print(train_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compiling the Model with specified callbacks and metrics:\n",
    "train_model.compile(optimizer = adam_optimizer, loss = \"categorical_crossentropy\", metrics = METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_44 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         multiple             1136        input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_45 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_46 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  multiple             380400      embedding_6[5][0]                \n",
      "                                                                 input_45[0][0]                   \n",
      "                                                                 input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_47 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_48 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  multiple             721200      lstm_18[5][0]                    \n",
      "                                                                 input_47[0][0]                   \n",
      "                                                                 input_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_49 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_50 (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  multiple             721200      lstm_19[5][0]                    \n",
      "                                                                 input_49[0][0]                   \n",
      "                                                                 input_50[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,823,936\n",
      "Trainable params: 1,823,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Defining the Inference Model\n",
    "# Needs the initial_state call argument for all the LSTM layers\n",
    "\n",
    "inference_input = Input(shape = (1,))\n",
    "lstm_1_hidden = Input(shape = (LSTM_UNITS,))\n",
    "lstm_1_cell = Input(shape = (LSTM_UNITS,))\n",
    "lstm_2_hidden = Input(shape = (LSTM_UNITS,))\n",
    "lstm_2_cell = Input(shape = (LSTM_UNITS,))\n",
    "lstm_3_hidden = Input(shape = (LSTM_UNITS,))\n",
    "lstm_3_cell = Input(shape = (LSTM_UNITS,))\n",
    "inference_emb_op = shared_embedding_layer(inference_input)\n",
    "inference_lstm_1_op, inference_lstm_1_hidden, inference_lstm_1_cell = shared_lstm_1(inference_emb_op, initial_state = [lstm_1_hidden, lstm_1_cell])\n",
    "inference_lstm_2_op, inference_lstm_2_hidden, inference_lstm_2_cell = shared_lstm_2(inference_lstm_1_op, initial_state = [lstm_2_hidden, lstm_2_cell])\n",
    "inference_lstm_3_op, inference_lstm_3_hidden, inference_lstm_3_cell = shared_lstm_3(inference_lstm_2_op, initial_state = [lstm_3_hidden, lstm_3_cell])\n",
    "inference_dense_1_op = shared_dense_1(inference_lstm_3_op)\n",
    "inference_model_op = shared_dense_op(inference_dense_1_op)\n",
    "\n",
    "inputs_list = [inference_input, lstm_1_hidden, lstm_1_cell, lstm_2_hidden, lstm_2_cell, lstm_3_hidden, lstm_3_cell]\n",
    "outputs_list = [inference_lstm_1_hidden, inference_lstm_1_cell, inference_lstm_2_hidden, inference_lstm_2_cell, inference_lstm_3_hidden, inference_lstm_3_cell]\n",
    "\n",
    "inference_model = Model(inputs = inputs_list, outputs = outputs_list)\n",
    "\n",
    "print(inference_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Pipeline Object Created!\n",
      "[+] Validation Data is read into volatile memory!\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data into volatile memory so we \n",
    "# don't have to re-perform all the calculation during each \n",
    "# validation test run.\n",
    "# Fitting the model might be very resource intensive\n",
    "# Due to data fetch from physical disk and prior pre-processing.\n",
    "input_pipeline = lstmNetworkInputSequence(None, \"./train/train_X.txt\", \"./train/train_Y.txt\", BATCH_SIZE, passwordTokenizer, MAX_LENGTH, VOCAB_SIZE)\n",
    "print(\"[+] Sequence Object Created!\")\n",
    "validation_data = getValidationData(\"./validation/X.txt\", \"./validation/Y.txt\", VOCAB_SIZE, passwordTokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "result = input_pipeline.__getitem__(2)\n",
    "x = result[0]\n",
    "y = result[1]\n",
    "print(\"[+] Validation Data is read into volatile memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 33) for input Tensor(\"input_57:0\", shape=(None, 33), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1) and (None, 1, 71) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-7493b250cceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2772\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2773\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 2774\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2703\u001b[0m     self._function_cache.arg_relaxed_shapes[rank_only_cache_key] = (\n\u001b[1;32m   2704\u001b[0m         relaxed_arg_shapes)\n\u001b[0;32m-> 2705\u001b[0;31m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[1;32m   2706\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[1;32m   2707\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /home/rm/anaconda3/envs/GRU/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1) and (None, 1, 71) are incompatible\n"
     ]
    }
   ],
   "source": [
    "train_model_history = train_model.fit(x = input_pipeline, epochs = 1, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "None\n",
      "./train/train_X.txt\n",
      "./train/train_Y.txt\n",
      "<keras_preprocessing.text.Tokenizer object at 0x7f4bdc9f5610>\n",
      "32\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(input_pipeline.batch_size)\n",
    "print(input_pipeline.train_passwords)\n",
    "print(input_pipeline.password_file)\n",
    "print(input_pipeline.password_file_y)\n",
    "print(input_pipeline.tokenizer)\n",
    "print(input_pipeline.max_length)\n",
    "print(input_pipeline.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376886"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pipeline.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 33)\n",
      "(32, 33, 71)\n"
     ]
    }
   ],
   "source": [
    "result = input_pipeline.__getitem__(1)\n",
    "print(result[0].shape)\n",
    "print(result[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l ./train/train_X.txt\n",
    "!wc -l ./train/train_Y.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.floor(3/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "[[ 1 18 18 ...  0  0  0]\n",
      " [ 1 13 17 ...  0  0  0]\n",
      " [ 1 12  7 ...  0  0  0]\n",
      " ...\n",
      " [ 1  3 10 ...  0  0  0]\n",
      " [ 1 23 27 ...  0  0  0]\n",
      " [ 1  6  5 ...  0  0  0]]\n",
      "(32, 33)\n",
      "(32, 33, 71)\n",
      "1/1 [==============================] - 0s 915us/step - loss: 1.2392 - categorical_accuracy: 0.7121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2392356395721436, 0.7121211886405945]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = input_pipeline.__getitem__(int(getLength(\"./train/train_X.txt\") / 32))\n",
    "print(type(result))\n",
    "x = result[0]\n",
    "print(x)\n",
    "print(x.shape)\n",
    "y = result[1]\n",
    "print(y.shape)\n",
    "train_model.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = Reshape(target_shape = (MAX_LENGTH+1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 33]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.array([1,2,3])\n",
    "print(arr1.shape)\n",
    "arr2 = np.array([1,2,3]).reshape(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(arr1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 31,  3, ...,  0,  0,  0],\n",
       "       [ 1, 31,  3, ...,  0,  0,  0],\n",
       "       [ 1, 31,  3, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 1, 31,  3, ...,  0,  0,  0],\n",
       "       [ 1, 31,  3, ...,  0,  0,  0],\n",
       "       [ 1, 31,  3, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 33)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ipykernel_GRU",
   "language": "python",
   "name": "ipykernel_gru"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
